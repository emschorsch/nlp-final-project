\documentclass[11pt]{article}
\usepackage{cs65f12}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
\restylefloat{table}
\setlength\titlebox{6.5cm}    % Expanding the titlebox

\title{Sentiment Classification and Analysis in Twitter}

\author{Justin Cosentino\\
  Department of Computer Science\\
  Swarthmore College\\
  Swarthmore, PA 19081\\
  {\tt jcosent1@swarthmore.edu}  
  \And                            
  Emanuel Schorsch\\                 
  Department of Computer Science\\
  Swarthmore College\\
  Swarthmore, PA 19081\\
  {\tt eschors1@swarthmore.edu}}

\date{}

\begin{document}
\maketitle
\begin{abstract}
As the popularity of microblogging drastically increases, it is evident that there is a vast amount of opinionated data and information available to assess the general sentiment of millions of Twitter users in regards to products, events, and people. We consider the problem of classifying the overall sentiment of subjects contained within tweets and the sentiment of a marked instance of a word or phrase within a given tweet as either positive, negative, neutral, or objective. In completing these tasks, we hope to further develop a public twitter sentiment corpus.\\

Using two Titter corpora provided by {\tt SemEval-2013}, we implemented and trained a naive Bayes classifier, a decision list classifier, and a subjectivity lexicon classifier. By using a bag-of-words framework and limited training data, we are able to successfully identify the sentiment of tweets. Our data suggests that the accuracy of our models will further increase as the size of our training corpora increase, and suggests that both naive Bayes and decision list classifiers trained on n-gram feature sets become far superior to the subjectivity lexicon classifier as the size of the training corpora increases. We find that that lack of slang and abbreviated words in the subjectivity lexicon account for this decrease in performance, and we propose creating a subjectivity lexicon that contains such language for the classification of tweets. We then conclude by examining and discussing the factors that make the sentiment classification of tweets so difficult. 

\end{abstract}

\section{Introduction}

\section{Related Works}
Sentiment analysis is a rapidly expanding field of natural language processing and a vast majority of these studies attempt to tackle a combination of two issues found in sentiment analysis: the identification of sentiment and then the classification of this expressed sentiment.  Many experiments attempt to classify the polarity of larger documents, specifically online reviews. For example, Pang and Lee analyze sentiment within online movie reviews~\cite{pang2002thumbs}. Using machine learning techniques, they study the effectiveness of a variety of features on sentiment detection. Their studies show that the presence of n-gram models is most useful in determining the polarity of a given review. Additionally, Pang and Lee found that the usage of parts of speech tagging and the presence of adjectives have little effect on improving the accuracy of their models and decreases this accuracy in some situations. Because of these findings, we have chosen not to experiment with features involving parts of speech. Pang and Lee also chose to experiment with word position. However, because the average tweet contains only eleven words, this is thought to have little effect on determining sentiment and is thus chosen not to be studied in our experiments~\cite{oconnor2010tweets}.

Additionally, there have been many recent studies that attempt to analyze the sentiment contained within tweets and other microblogging message formats. There have been many applications of Twitter sentiment classification, ranging from using microblogging as a form of sharing consumer opinions to linking Twitter sentiment and political public opinion~\cite{jansen2009twitter,oconnor2010tweets}. Microblogging features, such as hash-tags and emoticons, have played a key role in assisting in this analysis. Davidov et al.~\shortcite{davidov2010enhanced} proposed a supervised sentiment classification framework that utilized these features. Using hand tagged hash-tags and emoticons paired with feature types such as punctuation, patterns, and n-grams, they showed that by using a k-nearest neighbors strategy tweets can be paired with one or more of their labeled hash-tags, giving them the overall sentiment of that tweet. Similarly, Kouloumpis et al.~\shortcite{kouloumpis2011twitter} used microblogging, lexicon, n-gram, and part of speech features to evaluate twitter sentiment. This study confirmed that part of speech features result in a drop in performance in sentiment analysis.  Much like our study, these experiments attempt to label tweets as not only containing sentiment, but classifying this polarity as positive, negative, or neutral. 
\section{Methodology}

\subsection{Data}
A total of three different corpora were used in our experiments. Two of these corpora contained Twitter data that was used in the training and testing of our classifiers. The third data set was comprised of a sentiment lexicon which was used as a form of message classification.

\subsubsection*{Twitter Corpus}
As previously stated, Twitter is a microblogging service that allows users to post short, 140-character messages called tweets.  The two Twitter corpora used in this study were comprised of such tweets and acquired from the {\tt SemEval-2013} Competition site\footnote{Available at {\tt http://www.cs.york.ac.uk/{\\}semeval-2013/task2/}} using the provided script, which downloaded the tweets from the Twitter webpage. Some of these tweets were no longer available due to a user changing their privacy settings, deleting the designated tweet, or deleting their account. These tweets were ignored and not used within our study. Additionally, some tweets contained newline characters, which were removed from the tweet during the download process. Although the data found within these tweets was formatted differently, each corpus contained tweets covering a wide range of topics including entities, products, and events. The tweets found in the corpora were also exclusively written in English. The first corpus contained full tweets and the polarity tag for a marked instance of a word or phrase within each tweet while the second corpus contained full tweets and the polarity tag of a given topic found within the content of each tweet. There were a total of 2,376 tweet segments or phrases within the first corpus and 591 full tweets in the second and the polarity tags for each corpus were limited to 'positve', 'negative', 'neutral' and 'objective'. Although some of the tweets contained within each corpus were identical, the first corpus prompted users to only look at the given segment of the tweet. Each data set was used individually as both training and testing data throughout the study. The first 80\% of each corpus was constituted as training data while the remaining 20\% of the tweets were reserved for test data. 

\subsubsection*{Subjectivity Lexicon}
The third corpus used in our experiments contained subjectivity data relating to roughly 6,500 words. This lexicon was acquired from OpinionFinder, a system that performs subjectivity analysis~\cite{wilson2005opinionfinder}. Each word within this corpus has a corresponding polarity, strength of subjectivity, part-of-speech tag, stem value, and word length. For the purposes of this study, the strength of subjectivity, stem value, and word length were ignored. The polarity and part-of-speech tag were then used as means of classifying the sentiment value of tweets.

\subsection{Features}
In order to implement the sentiment classifiers such that they can label the given data, we used a number of features built from the bag-of-words model. This model is implemented as an unordered listing of features and does not take into account word order or word location. Using this model we create feature vectors that contain n-gram features present in a given tweet. The frequency of these features is also represented by the vector. In the implementation of the decision list and naive Bayes classifiers, unigrams, bigrams, and trigrams were used to train and classify data. In the subjectivity lexicon classifier, the individual words making up the tweet and their associated polarity make up the features of the vector. 

\subsection{Sentiment Classifiers}
In order to determine the sentiment of a given tweet, three algorithms were implemented: a naive Bayes classifier, a decision list classifier, and a subjectivity lexicon classifier.
\subsubsection*{Naive Bayes}
Using the Naive Bayes classifier, a given feature vector $\vec{f}$ is assigned
the polarity \^{s} such that 
$\hat{s} = \underset{{s}\in{S}}{\arg\max}P(s|\vec{f})$. This sentiment 
classifier is based upon and derived from a fundamental statistical rule 
called Bayes' law:
\[\hat{s} = \underset{{s}\in{S}}{\arg\max}{\frac{P(\vec{f}|s)P(s)}{P(\vec{f})}}.\]
Because $P(\vec{f})$ remains the same value across all possible polarities, it
does not assist in determing the value of $\hat{s}$. We then have:
\[\hat{s} = \underset{{s}\in{S}}{\arg\max}{P(\vec{f}|s)P(s)}.\]

However, given a feature vector $\vec{f}$ it is very unlikely that we will see
this exact feature again. Thus we naively assume that each $\vec{f}_i$ of 
$\vec{f}$ is independent of all other $\vec{f}_i$. Making this assumption
allows us to approximate $P(\vec{f}|s)$:
\[P(\vec{f}|s)\approx{\prod_{i=1}^n}P(f_i|s).\]

Thus, in estimating the probability of the vector $\vec{f}$ by finding the
product of the probabilities pertaining to each individual feature within
$\vec{f}$ given the polarity $\hat{s}$, we find the final equation for the
naive Bayes classifier:
\[\hat{s}= \underset{{s}\in{S}}{\arg\max}{{\prod_{i=1}^n}P(f_i|s)}P(s).\]

The maximum likelihood estimate of the probability of each possible sentiment
polarity is calculated by taking the count of the number of times a given 
feature occurs given the polarity over the number of times the feature occurs. 
This is represented by:
\[P(s_i) = \frac{count(s_i,w_j)}{count(w_j)}.\]

The probability of each feature given a sense is calculated in a similar manner such that:
\[P(f_i|s) = \frac{count(f_i,s)}{count(s)}.\]

Each of these equations utilizes Laplace add-one smoothing, allowing for non-zero probabilities to be assigned to words found in the test data that were not seen in the training sample. It is also important to note the using bigrams and trigrams as features in our naive Bayes classifier ignores the assumption that all
features within a feature vector are independent from one another. However, we are assuming that each n-gram within a tweet is independent from all other n-grams also contained within the tweet.

\subsubsection*{Decision List}
A decision list classifier is equivalent to simple case statements or an extended if-else statement. Decision lists generate a set of rules or conditions, for which there is a single classification associated. These rules are generated from tagged feature vectors and then scored and ordered based on their associated scores. Similar to the approach used by Yarowsky, each feature and value pair is treated as a rule~\cite{yarowsky1994decision}. In order to generate the best rule for each possible classification, the following equation is used:
\[\left|Log\left({\frac{P(Sense_i|f_i)}{P(\text{All other senses}|f_i)}}\right)\right|.\]
\indent Additionally, Laplace add-one smoothing is utilized when calculating the scores. Once these rules have been generated from the given feature sets and scored, the decision list creates a list of rules similar to those seen in Table 1. The decision list will then use these rules as long as their respective scores remain greater than or equal to one. At this point, the decision list will then proceed to classify words based on the most frequent sense of all tweets in the given training corpus.

\begin{table}[H]
  \begin{center}
  \begin{tabular}{| p{3cm} l l |}
  \hline
  Rule & & Polarity \\ \hline
  love & $\Rightarrow$ & Positive \\
  can't wait & $\Rightarrow$ & Positive \\
  looking forward & $\Rightarrow$ & Positive \\
  confirmed & $\Rightarrow$ & Objective \\
  crash & $\Rightarrow$ & Negative \\
  ... & $\Rightarrow$ & ... \\
  Score $<$ 1 & $\Rightarrow$ & MFS \\ \hline
  \end{tabular}
  \end{center}
  \caption{Example rules generated by the decision list after training on the second Twitter corpus.}
\end{table}

\subsubsection*{Subjectivity Lexicon}
A subjectivity lexicon was used to classify and determine the polarity of tweets. The subjectivity lexicon was acquired from OpinionFinder, a list containing roughly 6,500 words, their polarity, and the strength of their subjectivity. This lexicon was then used to determine the polarity of each word within the given tweet or tweet segment. The strength of each word, which was labeled as either strong or weak, was ignored for testing.

\indent Individual counts of the number of positive and negative words for each tweet were than kept. If a tweet or tweet segment contained more positive words than negative words, the tweet was then defined as having a positive polarity. If the tweet contained more negative words than positive words, the tweet was determined to have a negative polarity. However, if a tweet contained neither any positive words nor any negative words or if the count of positive and negative words were equal, the tweet was labeled as objective. Within the subjectivity lexicon classifier, words were not labeled as neutral.

\indent Because the subjectivity classifier requires no labeled tweets to be used as training data, the classifier was tested on all tweets within each corpus. However, in order to compare the results of the subjectivity lexicon classifier to the results of our other classifiers, the lexicon was also used to label only the test data used by all other classifiers.

\subsubsection*{Most Frequent Sense}
The most frequent sense (MFS) classifier is used as a baseline for comparison in our experiments. For each corpus, the classifier counts the number of occurrences for each polarity classification and then chooses that most frequently occurring tag. The classifier then labels all test data using this classification. This method is also used in the decision list if the list has exhausted all possible rules with a score greater than or equal to one.

\section{Results}
\begin{table*}[ht]
  \begin{center}
  \begin{tabular}{| l || l | l | l | l |}
  \hline
  Features & Naive Bayes & Decision List & Subjectivity Lexicon & MFS \\ 
  \hline \hline
  Unigrams &  &  & 54.17\% & 46.67\% \\  \hline
  Bigrams &  &  & N/A & 46.67\% \\   \hline
  Trigrams &  &  & N/A & 46.67\% \\  \hline
  \end{tabular}
  \end{center}
  \caption{Accuracy of classifiers labeling the sentiment of a given topic contained within a tweet. All classifiers were trained on 80\% of the second Twitter corpus and were then tested on the remaining 20\% of the data.}
\end{table*}

\section{Analysis}

\section{Conclusion}
\subsection{Future Work}

\bibliographystyle{cs65f12}
\bibliography{cs65f12}
\end{document}
