\documentclass[11pt]{article}
\usepackage{cs65f12}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
\usepackage{pgfplots}
\newcommand{\cou}{\textrm{count}}
\setlength\titlebox{6.5cm}    % Expanding the titlebox

\title{Sentiment Classification and Analysis in Twitter}

\author{Justin Cosentino\\
  Department of Computer Science\\
  Swarthmore College\\
  Swarthmore, PA 19081\\
  {\tt jcosent1@swarthmore.edu}  
  \And                            
  Emanuel Schorsch\\                 
  Department of Computer Science\\
  Swarthmore College\\
  Swarthmore, PA 19081\\
  {\tt eschors1@swarthmore.edu}}

\date{}

\begin{document}
\maketitle
\begin{abstract}
As the popularity of microblogging drastically increases, it is evident that there is a vast amount of opinionated data and information available to assess the general sentiment of millions of Twitter users in regards to products, events, and people. We consider the problem of classifying the overall sentiment of subjects contained within tweets and the sentiment of a marked instance of a word or phrase within a given tweet as either positive, negative, neutral, or objective. In completing these tasks, we hope to further develop a public twitter sentiment corpus.\\

Using two TWitter corpora provided by {\tt SemEval-2013}, we implemented and trained a naive Bayes classifier, a decision list classifier, and a subjectivity lexicon classifier. By using a bag-of-words framework and limited training data, we are able to successfully identify the sentiment of tweets. Our data suggests that the accuracy of our models will further increase as the size of our training corpora increase, and suggests that both naive Bayes and decision list classifiers trained on n-gram feature sets become far superior to the subjectivity lexicon classifier as the size of the training corpora increases. We find that that lack of slang and abbreviated words in the subjectivity lexicon account for this decrease in performance, and we propose creating a subjectivity lexicon that contains such language for the classification of tweets. We then conclude by examining and discussing the factors that make the sentiment classification of tweets so difficult. 

\end{abstract}

\section{Introduction}
To answer the question of how a given population feels about an issue it is usually necessary to conduct some kind of poll. People need to be surveyed and surveyors need to be hired to do the surveying and other laborious tasks. Ideally, instead we would be able to harvest the abundant information on the internet to find our answer. 

In recent years we have seen an explosion in social media use. Facebook and Twitter are two prime examples of platforms which millions of different people use on a daily basis. These platforms are perfectly positioned to answer the kinds of questions previously addressed by polls. Since everyone from your teacher to Obama is using these social networking platforms we have access to the opinions of people from a large variety of backgrounds. We choose to focus on Twitter as it is geared exclusively towards the expression of people¿s thoughts.  

Twitter is a microblogging platform which allows users to broadcast, or tweet, messages (tweets) of up to 140 characters to their followers. Since tweets must be so short they are almost always about a single topic, with usually only a single sentiment expressed. While Twitter does have a significant portion of non english speaking users, for the purposes of this paper we restrict our attention to tweets in english. One nice feature of Twitter is that tweets are labeled by hashtags. This makes it simple to search for a specific hashtag such as \#Toyota which will yield all tweets that are self-classified as relating to Toyotas. Given the enormous volume of tweets per day (340 million \footnote{{\tt http://blog.twitter.com/2012/03/twitter{\\}-turns-six.html}}) it is possible to find a sufficiently large sample of tweets about most topics, even relatively obscure ones.

There remain issues with ensuring that Twitter users represent a sufficiently random population but the issue of automated sentiment extraction looms much larger. In this paper we first discuss how to capture the sentiment of the component phrases of the tweets. This is useful as a building block which can hopefully aid in deciphering the sentiment of a tweet as a whole. Naturally we then consider the tweet as a whole and the overall sentiment of the tweet with respect to the topic as defined by a hashtag. A system which could perform well on this task would be invaluable for anyone studying public opinion. This would be immediately applicable to product managers and campaign managers as well as many researchers.

We found that simple features were sufficient to perform significantly above the baseline. It was interesting to see that having statistics on the prevalence of individual words among different senses was very powerful. We will discuss three different linguistic techniques we used to categorize tweets into their appropriate sense.

\section{Related Works}
Sentiment analysis is a rapidly expanding field of natural language processing and a vast majority of these studies attempt to tackle a combination of two issues found in sentiment analysis: the identification of sentiment and then the classification of this expressed sentiment.  Many experiments attempt to classify the polarity of larger documents, specifically online reviews. For example, Pang and Lee analyze sentiment within online movie reviews~\cite{pang2002thumbs}. Using machine learning techniques, they study the effectiveness of a variety of features on sentiment detection. Their studies show that the presence of n-gram models is most useful in determining the polarity of a given review. Additionally, Pang and Lee found that the usage of parts of speech tagging and the presence of adjectives have little effect on improving the accuracy of their models and decreases this accuracy in some situations. Because of these findings, we have chosen not to experiment with features involving parts of speech. Pang and Lee also chose to experiment with word position. However, because the average tweet contains only eleven words, this is thought to have little effect on determining sentiment and is thus chosen not to be studied in our experiments~\cite{oconnor2010tweets}.

Additionally, there have been many recent studies that attempt to analyze the sentiment contained within tweets and other microblogging message formats. There have been many applications of Twitter sentiment classification, ranging from using microblogging as a form of sharing consumer opinions to linking Twitter sentiment and political public opinion~\cite{jansen2009twitter,oconnor2010tweets}. Microblogging features, such as hash-tags and emoticons, have played a key role in assisting in this analysis. Davidov et al.~\shortcite{davidov2010enhanced} proposed a supervised sentiment classification framework that utilized these features. Using hand tagged hash-tags and emoticons paired with feature types such as punctuation, patterns, and n-grams, they showed that by using a k-nearest neighbors strategy tweets can be paired with one or more of their labeled hash-tags, giving them the overall sentiment of that tweet. Similarly, Kouloumpis et al.~\shortcite{kouloumpis2011twitter} used microblogging, lexicon, n-gram, and part of speech features to evaluate twitter sentiment. This study confirmed that part of speech features result in a drop in performance in sentiment analysis.  Much like our study, these experiments attempt to label tweets as not only containing sentiment, but classifying this polarity as positive, negative, or neutral. 
\section{Methodology}

\subsection{Data}
A total of three different corpora were used in our experiments. Two of these corpora contained Twitter data that was used in the training and testing of our classifiers. The third data set was comprised of a sentiment lexicon which was used as a form of message classification.

\subsubsection*{Twitter Corpus}
As previously stated, Twitter is a microblogging service that allows users to post short, 140-character messages called tweets.  The two Twitter corpora used in this study were comprised of such tweets and acquired from the {\tt SemEval-2013} Competition site\footnote{Available at {\tt http://www.cs.york.ac.uk/{\\}semeval-2013/task2/}} using the provided script, which downloaded the tweets from the Twitter webpage. Some of these tweets were no longer available due to a user changing their privacy settings, deleting the designated tweet, or deleting their account. These tweets were ignored and not used within our study. Additionally, some tweets contained newline characters, which were removed from the tweet during the download process. Although the data found within these tweets was formatted differently, each corpus contained tweets covering a wide range of topics including entities, products, and events. The tweets found in the corpora were also exclusively written in English. The first corpus contained full tweets and the polarity tag for a marked instance of a word or phrase within each tweet while the second corpus contained full tweets and the polarity tag of a given topic found within the content of each tweet. There were a total of 2,377 tweet segments or phrases within the first corpus and 592 full tweets in the second and the polarity tags for each corpus were limited to 'positve', 'negative', 'neutral' and 'objective'. Although some of the tweets contained within each corpus were identical, the first corpus prompted users to only look at the given segment of the tweet. Each data set was used individually as both training and testing data throughout the study. The first 80\% of each corpus was constituted as training data while the remaining 20\% of the tweets were reserved for test data. 

\subsubsection*{Subjectivity Lexicon}
The third corpus used in our experiments contained subjectivity data relating to roughly 6,500 words. This lexicon was acquired from OpinionFinder, a system that performs subjectivity analysis~\cite{wilson2005opinionfinder}. Each word within this corpus has a corresponding polarity, strength of subjectivity, part-of-speech tag, stem value, and word length. For the purposes of this study, the strength of subjectivity, stem value, and word length were ignored. The polarity and part-of-speech tag were then used as means of classifying the sentiment value of tweets.

\subsection{Features}
In order to implement the sentiment classifiers such that they can label the given data, we used a number of features built from the bag-of-words model. This model is implemented as an unordered listing of features and does not take into account word order or word location. Using this model we create feature vectors that contain n-gram features present in a given tweet. The frequency of these features is also represented by the vector. In the implementation of the decision list and naive Bayes classifiers, unigrams, bigrams, and trigrams were used to train and classify data. In the subjectivity lexicon classifier, the individual words making up the tweet and their associated polarity make up the features of the vector. 

\subsection{Sentiment Classifiers}
In order to determine the sentiment of a given tweet, three algorithms were implemented: a naive Bayes classifier, a decision list classifier, and a subjectivity lexicon classifier.
\subsubsection*{Naive Bayes}
Using the Naive Bayes classifier, a given feature vector $\vec{f}$ is assigned
the polarity \^{s} given by: 
$$\hat{s} = \underset{{s}\in{S}}{\arg\max}P(s|\vec{f})$$
Using Bayes rule we can rewrite this equation:
\[\hat{s} = \underset{{s}\in{S}}{\arg\max}{\frac{P(\vec{f}|s)P(s)}{P(\vec{f})}}.\]
Since $P(\vec{f})$ is constant across all polarities, it
does not affect the choice of $\hat{s}$ and we can discard it. This leaves us with:
\[\hat{s} = \underset{{s}\in{S}}{\arg\max}{P(\vec{f}|s)P(s)}.\]

However, it is very unlikely that we will see exactly our given feature vector $\vec{f}$ with $n$ features again. Thus we naively assume that each of the features in the vector are independent of each other. Under this assumption the below equality holds:
\[P(\vec{f}|s) = \prod_{i=1}^n P(f_i|s).\]

The probability of the vector $\vec{f}$ under the assumption of independence is equal to the product of the independent probability of each feature in the vector. Substituting into our previous equation we arrive at our naive Bayes classifier:
\[\hat{s} = \underset{s \in S}{\arg\max}{{\prod_{i=1}^n}P(f_i|s)}P(s).\]

The probability of ${f_i}$ which we use is the maximum likelihood estimate.This is computed by the number of times $f_i$ occurs as sense $s$ over the number of times $f_i$ occurs total. 
This is represented by:
\[P(s_i) = \frac{\cou(f_i,s)}{\cou(f_i)}.\]

The probability of a specific sense $s$ is calculated in a similar manner:
$$P(f_i|s) = \frac{\cou(s)}{\underset{s_i \in S}{\sum} \cou(s_i)}.$$

The feature we relied on most heavily was a unigram model. However, we also implemented bigram and trigram models to see if adding those features to the naive Bayes classifier would improve performance. While higher order n-gram models blatantly violate the assumption of conditional indpendence the naive Bayes classifier still performs well.

\subsubsection*{Decision List}
A decision list classifier is equivalent to simple case statements or an extended if-else statement. Decision lists generate a set of rules or conditions, for which there is a single classification associated. These rules are generated from tagged feature vectors and then scored and ordered based on their associated scores. Similar to the approach used by Yarowsky, each feature and value pair is treated as a rule~\cite{yarowsky1994decision}. In order to generate the best rule for each possible classification, the following equation is used:
\[\left|\log\left({\frac{P(Sense_i|f_i)}{P(\text{All other senses}|f_i)}}\right)\right|.\]
\indent Additionally, Laplace add-one smoothing is utilized when calculating the scores. Once these rules have been generated from the given feature sets and scored, the decision list creates a list of rules similar to those seen in Table 1. The decision list will then use these rules as long as their respective scores remain greater than or equal to one. At this point, the decision list will then proceed to classify words based on the most frequent sense of all tweets in the given training corpus.

\begin{table}[H]
  \begin{center}
  \begin{tabular}{| p{3cm} l l |}
  \hline
  Rule & & Polarity \\ \hline
  love & $\Rightarrow$ & Positive \\
  can't wait & $\Rightarrow$ & Positive \\
  looking forward & $\Rightarrow$ & Positive \\
  confirmed & $\Rightarrow$ & Objective \\
  crash & $\Rightarrow$ & Negative \\
  ... & $\Rightarrow$ & ... \\
  Score $<$ 1 & $\Rightarrow$ & MFS \\ \hline
  \end{tabular}
  \end{center}
  \caption{Example rules generated by the decision list after training on the second Twitter corpus.}
\end{table}

\subsubsection*{Subjectivity Lexicon}
A subjectivity lexicon was used to classify and determine the polarity of tweets. The subjectivity lexicon was acquired from OpinionFinder, a list containing roughly 6,500 words, their polarity, and the strength of their subjectivity. This lexicon was then used to determine the polarity of each word within the given tweet or tweet segment. The strength of each word, which was labeled as either strong or weak, was ignored for testing.

\indent Two variations of the subjectivity classifier were implemented. In both, individual counts of the number of positive and negative words for each tweet were then kept. If a tweet or tweet segment contained more positive words than negative words, the tweet was then defined as having a positive polarity. If the tweet contained more negative words than positive words, the tweet was determined to have a negative polarity. However in the first classifier, if a tweet contained neither any positive words nor any negative words or if the count of positive and negative words were equal, the tweet was labeled as objective. Within this subjectivity lexicon classifier, words were not labeled as neutral. If a tweet contained neither positive words nor negative words or contained an equal number of each polarity, the second subjectivity lexicon classifier would then determine the polarity of the tweet to be the most frequent sense, or polarity, contained within the training data. This second classifier will be referred to as the MFS subjectivity lexicon classifier.

\indent Because the first subjectivity classifier requires no labeled tweets to be used as training data, this classifier was tested on all tweets within each corpus. However, in order to compare the results of the subjectivity lexicon classifier to the results of our other classifiers, the lexicon was also used to label only the test data used by all other classifiers. The MFS subjectivity lexicon classifier was trained and tested on the same data as the decision list and naive Bayes classifiers. 

\subsubsection*{Most Frequent Sense}
The most frequent sense (MFS) classifier is used as a baseline for comparison in our experiments. For each corpus, the classifier counts the occurrences of each sense. The classifier then labels all test data with the sense with the highest count. This method is also used in the decision list if the list has no more rules with scores greater than or equal to one.

\section{Results}
\begin{table*}[htb!]
  \centering
  \begin{tabular}{| l || l | l | l | l | l |}
  \hline
  Features & Naive Bayes & Decision List & Subj. Lexicon & MFS Subj. Lexicon & MFS \\ 
  \hline \hline
  Unigrams              & 59.32\%      & \bf{58.66}\% & \bf{52.28}\% & \bf{55.47\%} & 52.12\% \\  \hline
  Bigrams               & 58.13\%      & 52.12\%      & N/A          & N/A          & 52.12\% \\  \hline
  Trigrams              & 52.20\%      & 52.12\%      & N/A          & N/A          & 52.12\% \\  \hline
  Bigrams \& Unigrams   & \bf{59.66}\% & 58.49\%      & N/A          & N/A          & 52.12\% \\  \hline
  Bi, Tri, \& Unigrams  & 58.48\%      & 58.49\%      & N/A          & N/A          & 52.12\% \\  \hline
  \end{tabular}
  \caption{Average accuracy of classifiers labeling the sentiment of a given topic contained within a tweet using fivefold cross-validation on the second Twitter corpus.}
\end{table*}

\begin{table*}[htp!]
  \centering
  \begin{tabular}{| l || l | l | l | l| l |}
  \hline
  Features & Naive Bayes & Decision List & Subj. Lexicon & MFS Subj. Lexicon & MFS \\ 
  \hline \hline
  Unigrams              & 64.84\%      & \bf{64.12\%} & \bf{44.98\%} & \bf{44.98\%} & 64.12\% \\  \hline
  Bigrams               & 64.76\%      & 64.12\%      & N/A          & N/A          & 64.12\% \\  \hline
  Trigrams              & 64.38\%      & 64.12\%      & N/A          & N/A          & 64.12\% \\  \hline
  Bigrams \& Unigrams   & \bf{65.56\%} & 64.12\%      & N/A          & N/A          & 64.12\% \\  \hline
  Bi, Tri, \& Unigrams  & \bf{65.56\%} & 64.12\%      & N/A          & N/A          & 64.12\% \\  \hline  
  \end{tabular}
  \caption{Average accuracy of classifiers labeling the sentiment of a subset of a tweet contained within a tweet using fivefold cross-validation on the first Twitter corpus.}
\end{table*}

\subsection{Tweet Polarity Classification}
The classification accuracies for determining the polarity of a given subject found within a tweet are shown in Table 2. These entries represent that average accuracy of each classifier when trained on the given feature using fivefold cross-validation. It is evident that bag-of-unigram features provided the best classification results, with each classifier surpassing the most frequent sense baseline of 52.12\%.

\subsubsection*{Decision List}
The decision list classifier did very well when training on some variety of unigrams, but only managed to achieve the same accuracy as the most frequent sense baseline when trained on only bigrams or trigrams. It is important to note that the rules generated by the decision list when trained on these two feature sets were rarely seen again, resulting the in the decision list classifying almost all tweets with the most frequent sense. However, when these features were paired with unigram features, the accuracy of the decision list increased significantly. This data suggests that unigram features are the most effective feature sets to use when training and testing on small data sets.

\subsubsection*{Naive Bayes}
The naive Bayes classifier was the most consistent classifier and generated the highest average accuracies. When trained on unigram feature sets, this classifier had similar accuracies to the decision list classifier. However, this classifier was able to score much better than both the decision list and the most frequent sense baseline when trained on bag-of-bigram features. This suggests that the naive Bayes classifier is more adept at determining the polarity of tweets when trained on very little data compared to the other classifiers tested.

\subsubsection*{Subjectivity Lexicon}
The subjectivity lexicon had the lowest accuracy of the three classifiers, scoring just above the average most frequent baseline. Although the lexicon was able to correctly determine the polarity of a given word, it was not able to account for sarcasm or negation. This resulted in the incorrect classification of tweets. It is important to note that the most frequent sense subjectivity classifier had a better average accuracy than both the most frequent sense and subjectivity lexicon classifiers, but not as high as either the naive bayes nor decision list classifiers. 

\subsection{Contextual Polarity Classification}

\begin{figure}[htb!]
  \centering
  \begin{tikzpicture}
  \begin{axis}[
    xlabel=Training Size (Percent of Corpus),
    ylabel=Accuracy,
    width = 7cm
  ]
  \addlegendentry{Most Freq. Sense};  
  \addplot[color=red, mark=*] coordinates {
    (10,46.67)
    (20,46.67)
    (30,46.67)
    (40,46.67)
    (50,46.67)
    (60,46.67)
    (70,46.67)
    (80,46.67)
  };
  \addlegendentry{Subj. Lexicon};  
  \addplot[color=blue, mark=x] coordinates {
    (10,54.17)
    (20,54.17)
    (30,54.17)
    (40,54.17)
    (50,54.17)
    (60,54.17)
    (70,54.17)
    (80,54.17)
  };
  \addlegendentry{MFS Subj. Lexicon};  
  \addplot[color=yellow, mark=*] coordinates {
    (10,53.33)
    (20,53.33)
    (30,53.33)
    (40,53.33)
    (50,53.33)
    (60,53.33)
    (70,53.33)
    (80,53.33)
  };
  \addlegendentry{DL: Unigrams};  
  \addplot[color=green, mark=+] coordinates {
    (10,45.83)
    (20,45.83)
    (30,52.50)
    (40,49.17)
    (50,52.50)
    (60,55.83)
    (70,57.50)
    (80,58.33)
  };
  \addlegendentry{NB: Unigrams};  
  \addplot[color=black, mark = diamond] coordinates {
    (10,43.70)
    (20,50.42)
    (30,46.22)
    (40,46.22)
    (50,52.10)
    (60,52.94)
    (70,54.62)
    (80,57.14)
  };
  \addplot[color=white] coordinates {
    (10,90)
  };
  \end{axis}
  \end{tikzpicture}
  \caption{Accuracy of classifiers labeling the sentiment of a given topic contained within a tweet. Each classifier was trained on the indicated percent of the second Twitter corpus and were then tested on the last 20\% of the data.}
\end{figure}

\subsection{Effects of Limited Training on Accuracy}
In addition to analyzing the ability of each classifier to determine the polarity of subjects and segments of tweets, we experimented with limiting the amount of training data available to each classifier. As shown in Figure 1, each classifier was given a percentage of training data from the second corpus and then used to classify the polarity of a designated test data set. For this experiment, twenty-percent of the tweets from the second Twitter corpus were reserved as test data. Each classifier then was trained on another percentage of the corpus, which increased by ten-percent each step. As the data incremented, the most frequent sense and both subjectivity lexicon classifiers remained at a constant accuracy. However, we can see an increase in accuracy in both the naive bayes and decision list classifiers as the size of the training data increases. This suggests that as each model trains on more data, their accuracy will continue to increase. 
\section{Analysis}

\section{Conclusion}
\subsection{Future Work}
One shortcoming to our sentiment lexicon is that the sentiment words weren't derived from tweets. This is a large problem since word usage changes dramatically from written text to internet micro-blogging. In order to account for slang, emoticons, and other idiosyncracies of Twitter it would be interesting to implement a bootstrapping algorithm \cite{Riloffbootstrapping}. This would allow us to extract sentiment words directly from training data increasing the accuracy of the sentiment lexicon.

\bibliographystyle{cs65f12}
\bibliography{cs65f12}
\end{document}
